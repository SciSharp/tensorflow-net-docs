# ä»‹ç»

## å‡­å•¥é€‰ C# å’Œ F# ç‰ˆæœ¬çš„ï¼Ÿ

é•¿å¾—å¾ˆåƒå•Šï¼Œå‡ ä¹æ²¡å­¦ä¹ éš¾åº¦ï¼š

![pythn vs csharp](../../_media/syntax-comparision.png)

ç”¨äº†è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°±å¯ä»¥åœ¨ C# å’Œ F# é¡¹ç›®ä¸Šè·‘æœºå™¨å­¦ä¹ äº†ã€‚

<video style="width: 100%;" src="_media/csharp-vs-python-speed.mp4" type="video/mp4" controls autoplay loop>python vs csharp on speed</video>

> æ¯«æ— æ‚¬å¿µï¼Œ .NET ç‰ˆæœ¬ (å·¦è¾¹é‚£ä¸ª) å®Œçˆ† python ç‰ˆæœ¬ (å³è¾¹é‚£ä¸ª). ğŸ‘† ç”¨ TensorFlow SGD è·‘ 1 ä¸‡æ¬¡å¾ªç¯çš„çº¿æ€§å›å½’. (CPU)

![python vs csharp on speed and memory](../../_media/csharp_vs_python_speed_memory.jpg)

> TensorFlow.NET æœ‰ 2x é€Ÿåº¦å’Œ 1/4 ç©ºé—´å ç”¨ç›¸æ¯”è¾ƒ python ç‰ˆ. (TensorFlow.NET 0.20-preview2)

## ä¸ºå•¥ä¼˜äº TensorFlowSharp ï¼Ÿ

[TensorFlowSharp](https://www.nuget.org/packages/TensorFlowSharp/) æ²¡æœ‰å®Œæ•´çš„ä¸Šå±‚ API æ¥è®­ç»ƒæ¨¡å‹ã€‚ä½†æ˜¯ [TF.NET](https://github.com/SciSharp/TensorFlow.NET) çš„å¼€å‘æ´»åŠ¨ä»ç„¶éå¸¸æ´»è·ƒï¼ŒAPI ååˆ†é½å…¨ï¼Œç›®å‰å·²è¢«å¾®è½¯ä½œä¸ºå®˜æ–¹æœºå™¨å­¦ä¹ æ¡†æ¶çš„åº•å±‚ã€‚

| TensorFlow                 | tf native1.14 | tf native 1.15 | tf native 2.3 |
| -------------------------- | ------------- | -------------- | ------------- |
| tf.net 0.3x, tf.keras 0.2  |               |                | x             |
| tf.net 0.2x                |               | x              | x             |
| tf.net 0.15                | x             | x              |               |
| tf.net 0.14                | x             |                |               |

## å¿«é€Ÿå…¥é—¨

[è®°å¾—å…ˆå®‰è£…ä¾èµ–åŒ…](zh-cn/essentials/installation.md)

### C# ä¾‹å­

å°† TF.NET å’Œ Keras API å¯¼å…¥åˆ°ä½ çš„é¡¹ç›®å†…ï¼š

```csharp
using static Tensorflow.Binding;
using static Tensorflow.KerasApi;
```

çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ï¼š

```csharp
// è¶…å˜é‡
var training_steps = 1000;
var learning_rate = 0.01f;
var display_step = 100;

// æœ€ç»ˆæˆ‘ä»¬è¦æ‹Ÿåˆçš„ç›®æ ‡æ•£ç‚¹
var X = np.array(3.3f, 4.4f, 5.5f, 6.71f, 6.93f, 4.168f, 9.779f, 6.182f, 7.59f, 2.167f,
             7.042f, 10.791f, 5.313f, 7.997f, 5.654f, 9.27f, 3.1f);
var Y = np.array(1.7f, 2.76f, 2.09f, 3.19f, 1.694f, 1.573f, 3.366f, 2.596f, 2.53f, 1.221f,
             2.827f, 3.465f, 1.65f, 2.904f, 2.42f, 2.94f, 1.3f);
var n_samples = X.shape[0];

// åˆå§‹åŒ– weights å’Œ bias ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºæ„ä¸€ä¸‹ï¼Œåé¢è¦ç”¨éšæœºå‡½æ•°æˆ–è€… 0 åˆå§‹åŒ–ï¼‰
var W = tf.Variable(-0.06f, name: "weight");
var b = tf.Variable(-0.73f, name: "bias");
var optimizer = tf.optimizers.SGD(learning_rate);

// è®­ç»ƒ step æ¬¡
foreach (var step in range(1, training_steps + 1))
{
    // å‡ºç°åœ¨ä¸‹é¢è¿™ä¸ª tf.GradientTape èŠ±æ‹¬å·é‡Œé¢çš„æ‰€æœ‰ä¸œè¥¿éƒ½ä¼šè¢«è®°å½•ï¼Œç„¶åä¼šè¢«è‡ªåŠ¨æ±‚å¯¼
    using (var g = tf.GradientTape())
    {
        // çº¿æ€§å›å½’ çš„ å‰å‘ä¼ æ’­ å…¬å¼
        var pred = W * X + b;
        // å‡æ–¹è¯¯å·® ï¼ˆMSEï¼‰.
        var loss = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * n_samples);
        // æ˜¯æ—¶å€™åœæ­¢è®°å½• å‰å‘ä¼ æ’­ äº†
        // ç°åœ¨å¼€å§‹è®¡ç®—æ¢¯åº¦.
        var gradients = g.gradient(loss, (W, b));
    }

    // æ ¹æ®æ‰€ç»™çš„æ¢¯åº¦å€¼ï¼Œæ›´æ–° weights å’Œ bias. ï¼ˆåå‘ä¼ æ’­ï¼‰
    optimizer.apply_gradients(zip(gradients, (W, b)));

    // åœ¨å‘½ä»¤è¡Œè¾“å‡ºä¸­é—´ç»“æœ
    if (step % display_step == 0)
    {
        pred = W * X + b;
        loss = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * n_samples);
        print($"step: {step}, loss: {loss.numpy()}, W: {W.numpy()}, b: {b.numpy()}");
    }
}
```

ç”¨ `Keras` çš„ API è¯•è¯•ç®€å•çš„ `ResNet`ï¼š

```csharp
// input layer
var inputs = keras.Input(shape: (32, 32, 3), name: "img");

// å·ç§¯å±‚
var x = layers.Conv2D(32, 3, activation: "relu").Apply(inputs);
x = layers.Conv2D(64, 3, activation: "relu").Apply(x);
var block_1_output = layers.MaxPooling2D(3).Apply(x);

x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(block_1_output);
x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(x);
var block_2_output = layers.add(x, block_1_output);

x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(block_2_output);
x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(x);
var block_3_output = layers.add(x, block_2_output);

x = layers.Conv2D(64, 3, activation: "relu").Apply(block_3_output);
x = layers.GlobalAveragePooling2D().Apply(x);
x = layers.Dense(256, activation: "relu").Apply(x);
x = layers.Dropout(0.5f).Apply(x);

// output layer
var outputs = layers.Dense(10).Apply(x);

// å¼€å§‹å®šä¹‰ keras æ¨¡å‹
model = keras.Model(inputs, outputs, name: "toy_resnet");
model.summary();

// å°† keras æ¨¡å‹ ç¼–è¯‘è¿› tensorflow çš„é™æ€å›¾
model.compile(optimizer: keras.optimizers.RMSprop(1e-3f),
	loss: keras.losses.CategoricalCrossentropy(from_logits: true),
	metrics: new[] { "acc" });

// å‡†å¤‡ dataset
var ((x_train, y_train), (x_test, y_test)) = keras.datasets.cifar10.load_data();

// å¼€å§‹è®­ç»ƒ
model.fit(x_train[new Slice(0, 1000)], y_train[new Slice(0, 1000)], 
          batch_size: 64, 
          epochs: 10, 
          validation_split: 0.2f);
```

### F# ä¾‹å­

çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ï¼š

```fsharp
#r "nuget: TensorFlow.Net"
#r "nuget: TensorFlow.Keras"
#r "nuget: SciSharp.TensorFlow.Redist"
#r "nuget: NumSharp"

open NumSharp
open Tensorflow
open type Tensorflow.Binding
open type Tensorflow.KerasApi

let tf = New<tensorflow>()
tf.enable_eager_execution()

// è¶…å˜é‡
let training_steps = 1000
let learning_rate = 0.01f
let display_step = 100

// æœ€ç»ˆæˆ‘ä»¬è¦æ‹Ÿåˆçš„ç›®æ ‡æ•£ç‚¹
let X = 
    np.array(3.3f, 4.4f, 5.5f, 6.71f, 6.93f, 4.168f, 9.779f, 6.182f, 7.59f, 2.167f,
             7.042f, 10.791f, 5.313f, 7.997f, 5.654f, 9.27f, 3.1f)
let Y = 
    np.array(1.7f, 2.76f, 2.09f, 3.19f, 1.694f, 1.573f, 3.366f, 2.596f, 2.53f, 1.221f,
             2.827f, 3.465f, 1.65f, 2.904f, 2.42f, 2.94f, 1.3f)
let n_samples = X.shape.[0]

// åˆå§‹åŒ– weights å’Œ bias ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºæ„ä¸€ä¸‹ï¼Œåé¢è¦ç”¨éšæœºå‡½æ•°æˆ–è€… 0 åˆå§‹åŒ–ï¼‰
let W = tf.Variable(-0.06f,name = "weight")
let b = tf.Variable(-0.73f, name = "bias")
let optimizer = keras.optimizers.SGD(learning_rate)

// è®­ç»ƒ step æ¬¡
for step = 1 to  (training_steps + 1) do 
    // å‡ºç°åœ¨ä¸‹é¢è¿™ä¸ª tf.GradientTape çš„ use æ§åˆ¶åŸŸé‡Œé¢çš„æ‰€æœ‰ä¸œè¥¿éƒ½ä¼šè¢«è®°å½•ï¼Œç„¶åä¼šè¢«è‡ªåŠ¨æ±‚å¯¼
    use g = tf.GradientTape()
    // çº¿æ€§å›å½’ çš„ å‰å‘ä¼ æ’­ å…¬å¼
    let pred = W * X + b
    // å‡æ–¹è¯¯å·® ï¼ˆMSEï¼‰.
    let loss = tf.reduce_sum(tf.pow(pred - Y,2)) / (2 * n_samples)
    // æ˜¯æ—¶å€™åœæ­¢è®°å½• å‰å‘ä¼ æ’­ äº†
    // ç°åœ¨å¼€å§‹è®¡ç®—æ¢¯åº¦.
    let gradients = g.gradient(loss,struct (W,b))

    // æ ¹æ®æ‰€ç»™çš„æ¢¯åº¦å€¼ï¼Œæ›´æ–° weights å’Œ bias. ï¼ˆåå‘ä¼ æ’­ï¼‰
    optimizer.apply_gradients(zip(gradients, struct (W,b)))

    if (step % display_step) = 0 then
        let pred = W * X + b
        let loss = tf.reduce_sum(tf.pow(pred-Y,2)) / (2 * n_samples)
        printfn $"step: {step}, loss: {loss.numpy()}, W: {W.numpy()}, b: {b.numpy()}"
```

## æ•´ç‚¹åˆ«çš„æ–‡æ¡£

æƒ³äº†è§£ç‚¹æ›´å¤šç»†èŠ‚ç©æ³•ï¼Ÿè¯•è¯•è¿™ä¸ª [C# TensorFlow 2 å…¥é—¨æ•™ç¨‹](https://github.com/SciSharp/TensorFlow.NET-Tutorials)ã€‚
ä¸å¤Ÿåˆºæ¿€ï¼Œæƒ³æ¥æ•´ç‚¹è‹±æ–‡çš„ï¼Ÿ åˆ°è¿™é‡Œ [The Definitive Guide to Tensorflow.NETï¼ˆè‹±æ–‡ï¼‰](https://tensorflownet.readthedocs.io/en/latest/FrontCover.html)ã€‚

## ç›´æ¥ä¸Šä»£ç 

æ²¡é—®é¢˜ï¼Œ[ç‚¹è¿™é‡Œ](https://github.com/SciSharp/SciSharp-Stack-Examples)é©¬ä¸Šæœ‰ï¼
