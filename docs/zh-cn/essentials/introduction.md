# ä»‹ç»

## å‡­å•¥é€‰ C# å’Œ F# ç‰ˆæœ¬çš„ï¼Ÿ

é•¿å¾—å¾ˆåƒå•Šï¼Œå‡ ä¹æ²¡å­¦ä¹ éš¾åº¦ï¼š

![pythn vs csharp](../../_media/syntax-comparision.png)

ç”¨äº†è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°±å¯ä»¥åœ¨ C# å’Œ F# é¡¹ç›®ä¸Šè·‘æœºå™¨å­¦ä¹ äº†ã€‚

<video style="width: 100%;" src="_media/csharp-vs-python-speed.mp4" type="video/mp4" controls autoplay loop>python vs csharp on speed</video>

> æ¯«æ— æ‚¬å¿µï¼Œ .NET ç‰ˆæœ¬ (å·¦è¾¹é‚£ä¸ª) å®Œçˆ† python ç‰ˆæœ¬ (å³è¾¹é‚£ä¸ª). ğŸ‘† ç”¨ TensorFlow SGD è·‘ 1 ä¸‡æ¬¡å¾ªç¯çš„çº¿æ€§å›å½’. (CPU)

![python vs csharp on speed and memory](../../_media/csharp_vs_python_speed_memory.jpg)

> TensorFlow.NET æœ‰ 2x é€Ÿåº¦å’Œ 1/4 ç©ºé—´å ç”¨ç›¸æ¯”è¾ƒ python ç‰ˆ. (TensorFlow.NET 0.20-preview2)

ä¸¾ä¸ªä¾‹å­ï¼Œ [TensorFlowSharp](https://www.nuget.org/packages/TensorFlowSharp/) éœ€è¦åœ¨ python å±‚ä¹‹ä¸Šè·‘ C# ä»£ç ï¼Œä¸å¤Ÿé«˜æ•ˆã€‚ä½†æ˜¯ [TF.NET](https://github.com/SciSharp/TensorFlow.NET) å°±ç›´æ¥è·Ÿ Tensorflow çš„ C++ API æ‰“äº¤é“ï¼Œå¤Ÿå¿«ï¼

| TensorFlow                 | tf native1.14 | tf native 1.15 | tf native 2.3 |
| -------------------------- | ------------- | -------------- | ------------- |
| tf.net 0.3x, tf.keras 0.2  |               |                | x             |
| tf.net 0.2x                |               | x              | x             |
| tf.net 0.15                | x             | x              |               |
| tf.net 0.14                | x             |                |               |

## å¿«é€Ÿå…¥é—¨

[è®°å¾—å…ˆå®‰è£…ä¾èµ–åŒ…](zh-cn/essentials/installation.md)

### C# ä¾‹å­

å°† TF.NET å’Œ Keras API å¯¼å…¥åˆ°ä½ çš„é¡¹ç›®å†…ï¼š

```csharp
using static Tensorflow.Binding;
using static Tensorflow.KerasApi;
```

çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ï¼š

```csharp
// Parameters        
var training_steps = 1000;
var learning_rate = 0.01f;
var display_step = 100;

// Sample data
var train_X = np.array(3.3f, 4.4f, 5.5f, 6.71f, 6.93f, 4.168f, 9.779f, 6.182f, 7.59f, 2.167f,
             7.042f, 10.791f, 5.313f, 7.997f, 5.654f, 9.27f, 3.1f);
var train_Y = np.array(1.7f, 2.76f, 2.09f, 3.19f, 1.694f, 1.573f, 3.366f, 2.596f, 2.53f, 1.221f,
             2.827f, 3.465f, 1.65f, 2.904f, 2.42f, 2.94f, 1.3f);
var n_samples = train_X.shape[0];

// We can set a fixed init value in order to demo
var W = tf.Variable(-0.06f, name: "weight");
var b = tf.Variable(-0.73f, name: "bias");
var optimizer = tf.optimizers.SGD(learning_rate);

// Run training for the given number of steps.
foreach (var step in range(1, training_steps + 1))
{
    // Run the optimization to update W and b values.
    // Wrap computation inside a GradientTape for automatic differentiation.
    using var g = tf.GradientTape();
    // Linear regression (Wx + b).
    var pred = W * X + b;
    // Mean square error.
    var loss = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * n_samples);
    // should stop recording
    // Compute gradients.
    var gradients = g.gradient(loss, (W, b));

    // Update W and b following gradients.
    optimizer.apply_gradients(zip(gradients, (W, b)));

    if (step % display_step == 0)
    {
        pred = W * X + b;
        loss = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * n_samples);
        print($"step: {step}, loss: {loss.numpy()}, W: {W.numpy()}, b: {b.numpy()}");
    }
}
```

ç”¨ `Keras` çš„ API è¯•è¯•ç®€å•çš„ `ResNet`ï¼š

```csharp
// input layer
var inputs = keras.Input(shape: (32, 32, 3), name: "img");

// convolutional layer
var x = layers.Conv2D(32, 3, activation: "relu").Apply(inputs);
x = layers.Conv2D(64, 3, activation: "relu").Apply(x);
var block_1_output = layers.MaxPooling2D(3).Apply(x);

x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(block_1_output);
x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(x);
var block_2_output = layers.add(x, block_1_output);

x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(block_2_output);
x = layers.Conv2D(64, 3, activation: "relu", padding: "same").Apply(x);
var block_3_output = layers.add(x, block_2_output);

x = layers.Conv2D(64, 3, activation: "relu").Apply(block_3_output);
x = layers.GlobalAveragePooling2D().Apply(x);
x = layers.Dense(256, activation: "relu").Apply(x);
x = layers.Dropout(0.5f).Apply(x);

// output layer
var outputs = layers.Dense(10).Apply(x);

// build keras model
model = keras.Model(inputs, outputs, name: "toy_resnet");
model.summary();

// compile keras model in tensorflow static graph
model.compile(optimizer: keras.optimizers.RMSprop(1e-3f),
	loss: keras.losses.CategoricalCrossentropy(from_logits: true),
	metrics: new[] { "acc" });

// prepare dataset
var ((x_train, y_train), (x_test, y_test)) = keras.datasets.cifar10.load_data();

// training
model.fit(x_train[new Slice(0, 1000)], y_train[new Slice(0, 1000)], 
          batch_size: 64, 
          epochs: 10, 
          validation_split: 0.2f);
```

### F# ä¾‹å­

çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ï¼š

```fsharp
#r "nuget: TensorFlow.Net"
#r "nuget: TensorFlow.Keras"
#r "nuget: SciSharp.TensorFlow.Redist"
#r "nuget: NumSharp"

open NumSharp
open Tensorflow
open type Tensorflow.Binding
open type Tensorflow.KerasApi

let tf = New<tensorflow>()
tf.enable_eager_execution()

// Parameters
let training_steps = 1000
let learning_rate = 0.01f
let display_step = 100

// Sample data
let train_X = 
    np.array(3.3f, 4.4f, 5.5f, 6.71f, 6.93f, 4.168f, 9.779f, 6.182f, 7.59f, 2.167f,
             7.042f, 10.791f, 5.313f, 7.997f, 5.654f, 9.27f, 3.1f)
let train_Y = 
    np.array(1.7f, 2.76f, 2.09f, 3.19f, 1.694f, 1.573f, 3.366f, 2.596f, 2.53f, 1.221f,
             2.827f, 3.465f, 1.65f, 2.904f, 2.42f, 2.94f, 1.3f)
let n_samples = train_X.shape.[0]

// We can set a fixed init value in order to demo
let W = tf.Variable(-0.06f,name = "weight")
let b = tf.Variable(-0.73f, name = "bias")
let optimizer = keras.optimizers.SGD(learning_rate)

// Run training for the given number of steps.
for step = 1 to  (training_steps + 1) do 
    // Run the optimization to update W and b values.
    // Wrap computation inside a GradientTape for automatic differentiation.
    use g = tf.GradientTape()
    // Linear regression (Wx + b).
    let pred = W * train_X + b
    // Mean square error.
    let loss = tf.reduce_sum(tf.pow(pred - train_Y,2)) / (2 * n_samples)
    // should stop recording
    // compute gradients
    let gradients = g.gradient(loss,struct (W,b))

    // Update W and b following gradients.
    optimizer.apply_gradients(zip(gradients, struct (W,b)))

    if (step % display_step) = 0 then
        let pred = W * train_X + b
        let loss = tf.reduce_sum(tf.pow(pred-train_Y,2)) / (2 * n_samples)
        printfn $"step: {step}, loss: {loss.numpy()}, W: {W.numpy()}, b: {b.numpy()}"
```

## æ•´ç‚¹åˆ«çš„æ–‡æ¡£

æƒ³äº†è§£ç‚¹æ›´å¤šç»†èŠ‚ç©æ³•ï¼Ÿè¯•è¯•è¿™ä¸ª [C# TensorFlow 2 å…¥é—¨æ•™ç¨‹](https://github.com/SciSharp/TensorFlow.NET-Tutorials)ã€‚
ä¸å¤Ÿåˆºæ¿€ï¼Œæƒ³æ¥æ•´ç‚¹è‹±æ–‡çš„ï¼Ÿ åˆ°è¿™é‡Œ [The Definitive Guide to Tensorflow.NETï¼ˆè‹±æ–‡ï¼‰](https://tensorflownet.readthedocs.io/en/latest/FrontCover.html)ã€‚

## ç›´æ¥ä¸Šä»£ç 

æ²¡é—®é¢˜ï¼Œ[ç‚¹è¿™é‡Œ](https://github.com/SciSharp/SciSharp-Stack-Examples)é©¬ä¸Šæœ‰ï¼
